{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2a4e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bn\n",
    "import re\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import os \n",
    "import numpy as np\n",
    "from PIL import ImageFilter, ImageEnhance\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cbf852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#extract data pdf to text file\n",
    "custom_config = r'--oem 3'\n",
    "\n",
    "def enhance_image_for_ocr(image):\n",
    "    gray = image.convert('L')\n",
    "    contrast_enhancer = ImageEnhance.Contrast(gray)\n",
    "    contrast = contrast_enhancer.enhance(2.0)\n",
    "    sharp = contrast.filter(ImageFilter.SHARPEN)\n",
    "    return sharp\n",
    "\n",
    "\n",
    "def pdf_to_images(pdf_path, dpi=300):\n",
    "    pages = convert_from_path(pdf_path, dpi=dpi)\n",
    "    enhanced_pages = []\n",
    "    for i in range(len(pages)):\n",
    "        if i>=4 and i<=18:\n",
    "            crop = pages[i].crop((0, 300, pages[i].width, pages[i].height-400))\n",
    "            enhanced_page = enhance_image_for_ocr(crop)\n",
    "            enhanced_pages.append(enhanced_page)\n",
    "    return enhanced_pages\n",
    "\n",
    "def extract_text_from_images(images):\n",
    "    full_text = \"\"\n",
    "    for i, img in enumerate(images):\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Processing page {i+1}...\")\n",
    "            text = pytesseract.image_to_string(img, lang='ben', config=custom_config)  # 'ben' = Bengali\n",
    "            clean_text = text.replace('\\xa0', ' ')  # Remove non-breaking spaces\n",
    "            clean_text = \"\\n\".join([line.strip() for line in clean_text.splitlines() if line.strip()])\n",
    "            #clean_text = clean_text.replace('\\n', ' ')\n",
    "            #full_text += f\"\\n\\n--- Page {i+1} ---\\n\\n{text}\"\n",
    "            full_text += clean_text\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d4c363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"HSC.pdf\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d8b049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 2...\n",
      "Processing page 3...\n",
      "Processing page 4...\n",
      "Processing page 5...\n",
      "Processing page 6...\n",
      "Processing page 7...\n",
      "Processing page 8...\n",
      "Processing page 9...\n",
      "Processing page 10...\n",
      "Processing page 11...\n",
      "Processing page 12...\n",
      "Processing page 13...\n",
      "Processing page 14...\n",
      "Processing page 15...\n",
      "✅ Done! Bangla text saved to 'extract_data2.txt'\n"
     ]
    }
   ],
   "source": [
    "text_file = \"extract_data2.txt\"\n",
    "if os.path.exists(text_file) == False:\n",
    "    images = pdf_to_images(pdf_file)\n",
    "    bangla_text = extract_text_from_images(images)\n",
    "    with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(bangla_text)\n",
    "\n",
    "    print(\"✅ Done! Bangla text saved to 'extract_data2.txt'\")\n",
    "else:\n",
    "    print(\"file exist please continue..........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "795568e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_and_vectorize_text(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text_content = file.read()\n",
    "\n",
    "        # --- Chunking ---\n",
    "        # Tokenize the text into individual sentences\n",
    "        text_content = text_content.replace(\"\\n\",\" \")\n",
    "        sentences = text_content.split(\"।\")\n",
    "        print(len(sentences))\n",
    "        sum_sentence = []\n",
    "        for i in range(0,len(sentences),8):\n",
    "            chunk = sentences[i:i+8]\n",
    "            sum_sentence.append(\"\".join(chunk))\n",
    "        return sum_sentence\n",
    "    except:\n",
    "        print(\"error\")\n",
    "\n",
    "def search_chunks_multiple_keywords(chunks, keywords):\n",
    "    # Normalize keywords to lowercase\n",
    "    keywords = [k.lower() for k in keywords]\n",
    "    \n",
    "    # Search for any of the keywords in each chunk\n",
    "    result = []\n",
    "    for chunk in chunks:\n",
    "        chunk_lower = chunk.lower()\n",
    "        if any(k in chunk_lower for k in keywords):\n",
    "            result.append(chunk)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52b59954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n"
     ]
    }
   ],
   "source": [
    "file_path = \"extract_data2.txt\"\n",
    "chunks = chunk_and_vectorize_text(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a40db2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de22f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chack data store in database\n",
    "# api_usage_examples.py\n",
    "import requests\n",
    "import json\n",
    "import uuid\n",
    "# Base URL of your FastAPI application\n",
    "BASE_URL = \"http://localhost:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0805a8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n"
     ]
    }
   ],
   "source": [
    "def create_sample_chunk(id,chunk,word, token_count):\n",
    "    \"\"\"Create a sample data chunk\"\"\"\n",
    "    sample_data = {\n",
    "        \"chunk_id\": id,\n",
    "        \"source_file\": \"sample_document.pdf\",\n",
    "        \"chunk_text\": chunk,\n",
    "        \"token_count\": token_count,\n",
    "        \"start_unit\": 0,\n",
    "        \"end_unit\": 46,\n",
    "        \"embedding_model\": \"text-embedding-ada-002\",\n",
    "        \"embedding\": f'{word}'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/chunks/\", json=sample_data)\n",
    "        if response.status_code == 200:\n",
    "            print(\"✅ Chunk created successfully!\")\n",
    "            chunk_data = response.json()\n",
    "            print(f\"Created chunk with ID: {chunk_data['id']}\")\n",
    "            return chunk_data['id']\n",
    "        else:\n",
    "            print(f\"❌ Error creating chunk: {response.text}\")\n",
    "            return None\n",
    "    except:\n",
    "        print(\"existing data.....\")\n",
    "\n",
    "\n",
    "\n",
    "for i, ch in enumerate(chunks):\n",
    "    st=bn.remove_stopwords(ch)\n",
    "    words_ch = bn.tokenizer(st) # or bn.tokenizer(text, 'word')\n",
    "    token_count = len(words_ch)\n",
    "    chunk_id = create_sample_chunk(i,ch,words_ch,token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "027dbd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_get(content,question):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=f'{content} read this content and give {question} this question answer. only give answer',\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "326811d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5874e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "১৫ বছর\n"
     ]
    }
   ],
   "source": [
    "st=bn.remove_stopwords(question)\n",
    "words = bn.tokenizer(st) # or bn.tokenizer(text, 'word')\n",
    "found_chunks = search_chunks_multiple_keywords(chunks, words)\n",
    "result_al = []\n",
    "for i, chunk in enumerate(found_chunks):\n",
    "    result_al.append(chunk)\n",
    "result = output_get(result_al,question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0dc223a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data get from dataset \n",
    "import requests\n",
    "\n",
    "# Get all chunk texts\n",
    "response = requests.get(\"http://localhost:8001/chunks/texts\")\n",
    "chunk_texts = response.json()\n",
    "list_chunk= []\n",
    "for chunk in chunk_texts:\n",
    "    list_chunk.append(chunk['chunk_text'])\n",
    "#print(list_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95dcc4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all data\n",
    "def get_all_chunks():\n",
    "    \"\"\"Get all data chunks\"\"\"\n",
    "    response = requests.get(f\"{BASE_URL}/chunks/\")\n",
    "    if response.status_code == 200:\n",
    "        chunks = response.json()\n",
    "        print(chunks)\n",
    "        print(f\"✅ Retrieved {len(chunks)} chunks\")\n",
    "        for chunk in chunks:\n",
    "            print(f\"  - ID: {chunk['id']}, Source: {chunk['source_file']}\")\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"❌ Error retrieving chunks: {response.text}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1715c870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\n",
      "বিয়ের প্রস্তাব চলাকালীন কল্যাণীর বয়স ছিল পনেরো।\n"
     ]
    }
   ],
   "source": [
    "def generate_bot_response(list_chunk,user_message: str) -> str:\n",
    "    user_message = user_message.lower().strip()\n",
    "    print(user_message)\n",
    "    st=bn.remove_stopwords(user_message)\n",
    "    words = bn.tokenizer(st) # or bn.tokenizer(text, 'word')\n",
    "    found_chunks = search_chunks_multiple_keywords(list_chunk, words)\n",
    "    result_al = []\n",
    "    for i, chunk in enumerate(found_chunks):\n",
    "        result_al.append(chunk)\n",
    "    result = output_get(result_al,user_message)\n",
    "    print(result)\n",
    "    return result\n",
    "question_ai = \"বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\"\n",
    "result = generate_bot_response(list_chunk,question_ai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
