{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2a4e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bn\n",
    "import re\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import os \n",
    "import numpy as np\n",
    "from PIL import ImageFilter, ImageEnhance\n",
    "from google import genai\n",
    "from typing import List, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "import psycopg2\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34bba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chack data store in database\n",
    "# api_usage_examples.py\n",
    "import requests\n",
    "import json\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cbf852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#extract data pdf to text file\n",
    "custom_config = r'--oem 3'\n",
    "\n",
    "def enhance_image_for_ocr(image):\n",
    "    gray = image.convert('L')\n",
    "    contrast_enhancer = ImageEnhance.Contrast(gray)\n",
    "    contrast = contrast_enhancer.enhance(2.0)\n",
    "    sharp = contrast.filter(ImageFilter.SHARPEN)\n",
    "    return sharp\n",
    "\n",
    "\n",
    "def pdf_to_images(pdf_path, dpi=300):\n",
    "    pages = convert_from_path(pdf_path, dpi=dpi)\n",
    "    enhanced_pages = []\n",
    "    for i in range(len(pages)):\n",
    "        if i>=4 and i<=18:\n",
    "            crop = pages[i].crop((0, 300, pages[i].width, pages[i].height-400))\n",
    "            enhanced_page = enhance_image_for_ocr(crop)\n",
    "            enhanced_pages.append(enhanced_page)\n",
    "    return enhanced_pages\n",
    "\n",
    "def extract_text_from_images(images):\n",
    "    full_text = \"\"\n",
    "    for i, img in enumerate(images):\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Processing page {i+1}...\")\n",
    "            text = pytesseract.image_to_string(img, lang='ben', config=custom_config)  # 'ben' = Bengali\n",
    "            clean_text = text.replace('\\xa0', ' ')  # Remove non-breaking spaces\n",
    "            clean_text = \"\\n\".join([line.strip() for line in clean_text.splitlines() if line.strip()])\n",
    "            #clean_text = clean_text.replace('\\n', ' ')\n",
    "            #full_text += f\"\\n\\n--- Page {i+1} ---\\n\\n{text}\"\n",
    "            full_text += clean_text\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d4c363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"HSC.pdf\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d8b049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exist please continue..........\n"
     ]
    }
   ],
   "source": [
    "text_file = \"extract_data2.txt\"\n",
    "if os.path.exists(text_file) == False:\n",
    "    images = pdf_to_images(pdf_file)\n",
    "    bangla_text = extract_text_from_images(images)\n",
    "    with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(bangla_text)\n",
    "\n",
    "    print(\"‚úÖ Done! Bangla text saved to 'extract_data2.txt'\")\n",
    "else:\n",
    "    print(\"file exist please continue..........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1db8cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_get(content,question):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=f'{content} read this content and give {question} this question answer. only give answer',\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795568e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_and_vectorize_text(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text_content = file.read()\n",
    "\n",
    "        # --- Chunking ---\n",
    "        # Tokenize the text into individual sentences\n",
    "        text_content = text_content.replace(\"\\n\",\" \")\n",
    "        sentences = text_content.split(\"‡•§\")\n",
    "        print(len(sentences))\n",
    "        sum_sentence = []\n",
    "        for i in range(0,len(sentences),8):\n",
    "            chunk = sentences[i:i+8]\n",
    "            sum_sentence.append(\"\".join(chunk))\n",
    "        return sum_sentence\n",
    "    except:\n",
    "        print(\"error\")\n",
    "\n",
    "def search_chunks_multiple_keywords(chunks, keywords):\n",
    "    # Normalize keywords to lowercase\n",
    "    keywords = [k.lower() for k in keywords]\n",
    "    \n",
    "    # Search for any of the keywords in each chunk\n",
    "    result = []\n",
    "    for chunk in chunks:\n",
    "        chunk_lower = chunk.lower()\n",
    "        if any(k in chunk_lower for k in keywords):\n",
    "            result.append(chunk)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52b59954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n"
     ]
    }
   ],
   "source": [
    "file_path = \"extract_data2.txt\"\n",
    "chunks = chunk_and_vectorize_text(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322e824",
   "metadata": {},
   "source": [
    "Method 1: Chucking and Vectorize using Langchain and Gemini LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f34666c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgvector extension ensured.\n",
      "[{'topic': '‡¶Æ‡ßÇ‡¶≤'}]\n",
      "‚úÖ Stored 36 chunks.\n",
      "\n",
      "üîç Results for: '‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?'\n"
     ]
    }
   ],
   "source": [
    "# Environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "DATABASE_URL = os.getenv(\"NEON_DATABASE_URL\")\n",
    "\n",
    "class TextToEmbeddingsPipeline:\n",
    "    def __init__(self, db_url: str, collection_name: str = \"documents\"):\n",
    "        self.db_url = db_url\n",
    "        self.collection_name = collection_name\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            google_api_key=GOOGLE_API_KEY,\n",
    "            model=\"models/embedding-001\"\n",
    "        )\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, chunk_overlap=200, length_function=len\n",
    "        )\n",
    "        self.vector_store = None\n",
    "        self._setup_pgvector()\n",
    "\n",
    "    def _setup_pgvector(self):\n",
    "        try:\n",
    "            conn = psycopg2.connect(self.db_url)\n",
    "            with conn:\n",
    "                with conn.cursor() as cur:\n",
    "                    cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "            print(\"pgvector extension ensured.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Database setup failed: {e}\")\n",
    "\n",
    "    def _init_vector_store(self, documents: List[Document]):\n",
    "        try:\n",
    "            return PGVector.from_documents(\n",
    "                documents=documents,\n",
    "                embedding=self.embeddings,\n",
    "                connection=self.db_url,\n",
    "                collection_name=self.collection_name,\n",
    "            )\n",
    "        except TypeError:\n",
    "            return PGVector.from_documents(\n",
    "                documents=documents,\n",
    "                embedding=self.embeddings,\n",
    "                connection_string=self.db_url,\n",
    "                collection_name=self.collection_name,\n",
    "            )\n",
    "\n",
    "    def process_texts(self, texts: List[str], metadatas: Optional[List[dict]] = None):\n",
    "        documents = [Document(page_content=text, metadata=(metadatas[i] if metadatas else {})) for i, text in enumerate(texts)]\n",
    "        chunks = self.text_splitter.split_documents(documents)\n",
    "\n",
    "        if self.vector_store is None:\n",
    "            self.vector_store = self._init_vector_store(chunks)\n",
    "        else:\n",
    "            self.vector_store.add_documents(chunks)\n",
    "\n",
    "        print(f\"‚úÖ Stored {len(chunks)} chunks.\")\n",
    "        return self.vector_store\n",
    "\n",
    "    def process_file(self, file_path: str, metadata: Optional[dict] = None):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            return self.process_texts([text], [metadata or {\"source\": file_path}])\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    def search(self, query: str, k: int = 3) -> List[Document]:\n",
    "        return self.vector_store.similarity_search(query, k=k) if self.vector_store else []\n",
    "\n",
    "    def search_with_score(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "        return self.vector_store.similarity_search_with_score(query, k=k) if self.vector_store else []\n",
    "\n",
    "\n",
    "def main(query):\n",
    "    pipeline = TextToEmbeddingsPipeline(db_url=DATABASE_URL, collection_name=\"my_documents\")\n",
    "\n",
    "    with open(\"extract_data2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = f.read().split(\"\\n\\n\")\n",
    "\n",
    "    metadatas = [{\"topic\": t.split()[0].lower()} for t in texts]\n",
    "    print(metadatas)\n",
    "\n",
    "    pipeline.process_texts(texts, metadatas)\n",
    "\n",
    "    \n",
    "    results = pipeline.search(query)\n",
    "\n",
    "    print(f\"\\nüîç Results for: '{query}'\")\n",
    "    ai_input_text = []\n",
    "    for i, doc in enumerate(results):\n",
    "        ai_input_text.append(doc.page_content)\n",
    "        #print(f\"{i+1}. {doc.page_content}\\n   Metadata: {doc.metadata}\")\n",
    "    join_txt = \" \".join(ai_input_text)\n",
    "    return join_txt\n",
    "    \n",
    "\n",
    "query = \"‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?\"\n",
    "if __name__ == \"__main__\":\n",
    "    join_txt=main(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f555c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡¶™‡ßç‡¶∞‡¶¶‡¶§‡ßç‡¶§ ‡¶Ö‡¶Ç‡¶∂‡ßá ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶ï‡ßã‡¶®‡ßã ‡¶§‡¶•‡ßç‡¶Ø ‡¶®‡ßá‡¶á‡•§\n"
     ]
    }
   ],
   "source": [
    "ai_result = output_get(join_txt,query)\n",
    "print(ai_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2477f4",
   "metadata": {},
   "source": [
    "Method 2: Chucking text in Sentence and store database in string. search question keyword in database and get text. find answer in this text using ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0805a8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n",
      "existing data.....\n"
     ]
    }
   ],
   "source": [
    "# Data store in neon database: db_neon\n",
    "def create_sample_chunk(id,chunk,word, token_count):\n",
    "    \"\"\"Create a sample data chunk\"\"\"\n",
    "    sample_data = {\n",
    "        \"chunk_id\": id,\n",
    "        \"source_file\": \"sample_document.pdf\",\n",
    "        \"chunk_text\": chunk,\n",
    "        \"token_count\": token_count,\n",
    "        \"start_unit\": 0,\n",
    "        \"end_unit\": 46,\n",
    "        \"embedding_model\": \"text-embedding-ada-002\",\n",
    "        \"embedding\": f'{word}'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/chunks/\", json=sample_data)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Chunk created successfully!\")\n",
    "            chunk_data = response.json()\n",
    "            print(f\"Created chunk with ID: {chunk_data['id']}\")\n",
    "            return chunk_data['id']\n",
    "        else:\n",
    "            print(f\"‚ùå Error creating chunk: {response.text}\")\n",
    "            return None\n",
    "    except:\n",
    "        print(\"existing data.....\")\n",
    "\n",
    "# Base URL of your FastAPI application\n",
    "BASE_URL = \"http://localhost:8001\"\n",
    "\n",
    "for i, ch in enumerate(chunks):\n",
    "    st=bn.remove_stopwords(ch)\n",
    "    words_ch = bn.tokenizer(st) # or bn.tokenizer(text, 'word')\n",
    "    token_count = len(words_ch)\n",
    "    chunk_id = create_sample_chunk(i,ch,words_ch,token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "326811d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb5ab5",
   "metadata": {},
   "source": [
    "Run By sending Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5874e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡ßß‡ß´ ‡¶¨‡¶õ‡¶∞\n"
     ]
    }
   ],
   "source": [
    "st=bn.remove_stopwords(question)\n",
    "words = bn.tokenizer(st) # or bn.tokenizer(text, 'word')\n",
    "found_chunks = search_chunks_multiple_keywords(chunks, words)\n",
    "result_al = []\n",
    "for i, chunk in enumerate(found_chunks):\n",
    "    result_al.append(chunk)\n",
    "result = output_get(result_al,question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33b190",
   "metadata": {},
   "source": [
    "Run by Get data from Neon dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dc223a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?\n",
      "‡¶™‡¶®‡ßá‡¶∞‡ßã‡•§\n"
     ]
    }
   ],
   "source": [
    "#data get from dataset \n",
    "import requests\n",
    "# Get all chunk texts\n",
    "response = requests.get(\"http://localhost:8001/chunks/texts\")\n",
    "chunk_texts = response.json()\n",
    "def generate_bot_response(list_chunk,user_message: str) -> str:\n",
    "    user_message = user_message.lower().strip()\n",
    "    print(user_message)\n",
    "    st=bn.remove_stopwords(user_message)\n",
    "    words = bn.tokenizer(st) # or bn.tokenizer(text, 'word')\n",
    "    found_chunks = search_chunks_multiple_keywords(list_chunk, words)\n",
    "    result_al = []\n",
    "    for i, chunk in enumerate(found_chunks):\n",
    "        result_al.append(chunk)\n",
    "    result = output_get(result_al,user_message)\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "list_chunk= []\n",
    "for chunk in chunk_texts:\n",
    "    list_chunk.append(chunk['chunk_text'])\n",
    "#print(list_chunk)\n",
    "\n",
    "question_ai = \"‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?\"\n",
    "result = generate_bot_response(list_chunk,question_ai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
